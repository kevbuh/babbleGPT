{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6482c5e",
   "metadata": {},
   "source": [
    "# Tiny Generative Pretrained Transformer\n",
    "\n",
    "Generating infinite Shakespeare by training a transformer based language model on all of Shakespeare's work. \n",
    "\n",
    "Lots of help from Arxiv and Andrej Karpathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc2c3890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.pyplot import plot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9036dd82",
   "metadata": {},
   "source": [
    "### Links\n",
    "\n",
    "Annotated Transformer - http://nlp.seas.harvard.edu/annotated-transformer/\n",
    "\n",
    "The Illustrated Tranformer - https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4e184",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Tiny models need tiny datasets. \n",
    "\n",
    "We are using the Tiny Shakespeare dataset, which is a concatenation of all the works of Shakespeare - https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3fd4a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-05 20:38:35--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: 'input.txt.1'\n",
      "\n",
      "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2023-02-05 20:38:36 (11.7 MB/s) - 'input.txt.1' saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8dc8e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3343995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of characters: {len(text)}\") # over a million characters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d751f261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 1000 words\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d31c345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters in set:\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Characters in set:\" + ''.join(chars))\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75a88f",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Real GPT's use tiktoken - https://github.com/openai/tiktoken\n",
    "\n",
    "Below is just a character level tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9a92f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translating characters into integers\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars)}\n",
    "itos = { i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]          # encoder: String -> List[Ints]\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: List[Ints] -> String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25e25fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7dd7984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello world!'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cddde155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Encoding the entire Shakespeare work\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc0d601",
   "metadata": {},
   "source": [
    "### Train and Validation Splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2caedb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 90% of the dataset to be train with 10% validation \n",
    "n = int(0.9 * len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f564f28",
   "metadata": {},
   "source": [
    "Putting the entire chuck of text into the transformer at once in computationally infeasible. So, we need to sample little chunks of the dataset, and feed that into the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "613e3f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e41540ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is tensor([18]), the target is: 47\n",
      "When the input is tensor([18, 47]), the target is: 56\n",
      "When the input is tensor([18, 47, 56]), the target is: 57\n",
      "When the input is tensor([18, 47, 56, 57]), the target is: 58\n",
      "When the input is tensor([18, 47, 56, 57, 58]), the target is: 1\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1]), the target is: 15\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is: 47\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size] # inputs to the transformer\n",
    "y = train_data[1:block_size+1] # targets for each position, offset by one \n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When the input is {context}, the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05a61936",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "406bbcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4 # how many sequences we process every forward and backwards pass\n",
    "block_size = 8 # maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split=='train' else val_data\n",
    "    \n",
    "    # randomly get indexes of dataset\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # torch.stacks get the one dimensional tensor like [18, 47, 56, 57, 58,  1, 15, 47]\n",
    "    # and puts them all together into a 4x8 tensor\n",
    "    x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "    return x.to(device),y.to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "707731c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]], device='mps:0')\n",
      "targets: torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]], device='mps:0')\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "x_batched, y_batched = get_batch('train')\n",
    "\n",
    "print(f\"inputs: {x_batched.shape}\") # a 4x8 array contains 32 indepenednt examples, each shown in new cell\n",
    "print(x_batched)\n",
    "print(f\"targets: {y_batched.shape}\")\n",
    "print(y_batched)\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b0f87fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is [24], the target is 43\n",
      "When input is [24, 43], the target is 58\n",
      "When input is [24, 43, 58], the target is 5\n",
      "When input is [24, 43, 58, 5], the target is 57\n",
      "When input is [24, 43, 58, 5, 57], the target is 1\n",
      "When input is [24, 43, 58, 5, 57, 1], the target is 46\n",
      "When input is [24, 43, 58, 5, 57, 1, 46], the target is 43\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43], the target is 39\n",
      "When input is [44], the target is 53\n",
      "When input is [44, 53], the target is 56\n",
      "When input is [44, 53, 56], the target is 1\n",
      "When input is [44, 53, 56, 1], the target is 58\n",
      "When input is [44, 53, 56, 1, 58], the target is 46\n",
      "When input is [44, 53, 56, 1, 58, 46], the target is 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39], the target is 58\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58], the target is 1\n",
      "When input is [52], the target is 58\n",
      "When input is [52, 58], the target is 1\n",
      "When input is [52, 58, 1], the target is 58\n",
      "When input is [52, 58, 1, 58], the target is 46\n",
      "When input is [52, 58, 1, 58, 46], the target is 39\n",
      "When input is [52, 58, 1, 58, 46, 39], the target is 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58], the target is 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1], the target is 46\n",
      "When input is [25], the target is 17\n",
      "When input is [25, 17], the target is 27\n",
      "When input is [25, 17, 27], the target is 10\n",
      "When input is [25, 17, 27, 10], the target is 0\n",
      "When input is [25, 17, 27, 10, 0], the target is 21\n",
      "When input is [25, 17, 27, 10, 0, 21], the target is 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1], the target is 54\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54], the target is 39\n"
     ]
    }
   ],
   "source": [
    "for batch in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = x_batched[batch, :t+1]\n",
    "        target = y_batched[batch,t]\n",
    "        \n",
    "        # all of these are independent, as far as the transformer is concerned\n",
    "        print(f\"When input is {context.tolist()}, the target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f69083",
   "metadata": {},
   "source": [
    "### Bigram Language Model\n",
    "\n",
    "The most simple baseline.\n",
    "\n",
    "A good loss function is negatively log likelihood loss aka in PyTorch as ```cross_entropy()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5ef80676",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5f1da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        # idx and targets are both (Batch, Time) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (Batch, Time, Channel) \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else: \n",
    "            # PyTorch expects (B, C, T), we need to convert\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx) # get the predictions\n",
    "            \n",
    "            # focus on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            \n",
    "            # sample from the distribition\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "53f7aab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " logits shape: torch.Size([32, 65]), loss: 4.878634452819824\n"
     ]
    }
   ],
   "source": [
    "model_0 = BigramLanguageModel(vocab_size).to(device)\n",
    "logits, loss = model_0(x_batched, y_batched)\n",
    "print(f\" logits shape: {logits.shape}, loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1e49b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SjbhKptLNsA:q,kSPw'aPGXE$i\n",
      "SpxnkXUzkzC-wJr.\n",
      "\n",
      ";u!Idl,obcHA-uQ'\n",
      "b3sZ 3KUD&j3Sq-'d'qb?Elnlh'nFbEQbduehT\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long).to(device)\n",
    "print(decode(model_0.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6d81e",
   "metadata": {},
   "source": [
    "### Training Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "85602dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model_0.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc396767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.394822120666504\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data\n",
    "    x_batched, y_batched = get_batch('train')\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = model_0(x_batched, y_batched)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4b96f6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Houreng; wou ino clY kn po thace;\n",
      "CETonourd? s llly s, ind anr JMESck der ait We we bueswars\n",
      "Pis ftw$VOH:\n",
      "ro le it h y,\n",
      "\n",
      "OULINBee, bysas w OLLORLonea merond s teathist storir; ms hen,-ovy isowisthis thay lororeilar tho thothatiory ad Is arourst masund m mpuld t ng heaie.\n",
      "RRer t t howoo nd saresquet\n"
     ]
    }
   ],
   "source": [
    "print(decode(model_0.generate(idx, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b809208",
   "metadata": {},
   "source": [
    "It's looks like it is kind of working!?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d368505c",
   "metadata": {},
   "source": [
    "### Self attention\n",
    "\n",
    "Tokens should not be able to use future tokens as context.\n",
    "\n",
    "So, we want to calculate the average of all vectors in all the previous n tokens, including n. We can use this using a bag of words https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a480e5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8 , 32\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f66c35cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bag_of_words = torch.zeros(B,T,C)\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_prev = x[b,:t+1] # (t,C)\n",
    "        x_bag_of_words[b,t] = torch.mean(x_prev, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa8678",
   "metadata": {},
   "source": [
    "The first row of these are the same, which makes sense because we have only looked at one word. The next rows change as we are then taking the average of 0...n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6100b961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8077e-01, -6.9988e-02, -3.5962e-01, -9.1520e-01,  6.2577e-01,\n",
       "          2.5510e-02,  9.5451e-01,  6.4349e-02,  3.6115e-01,  1.1679e+00,\n",
       "         -1.3499e+00, -5.1018e-01,  2.3596e-01, -2.3978e-01, -9.2111e-01,\n",
       "          1.5433e+00,  1.3488e+00, -1.3964e-01,  2.8580e-01,  9.6512e-01,\n",
       "         -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,  1.2603e-01,\n",
       "         -1.5627e+00, -1.1601e+00, -3.3484e-01,  4.4777e-01, -8.0164e-01,\n",
       "          1.5236e+00,  2.5086e+00],\n",
       "        [-6.6310e-01, -2.5128e-01,  1.0101e+00,  1.2155e-01,  1.5840e-01,\n",
       "          1.1340e+00, -1.1539e+00, -2.9840e-01, -5.0754e-01, -9.2392e-01,\n",
       "          5.4671e-01, -1.4948e+00, -1.2057e+00,  5.7182e-01, -5.9735e-01,\n",
       "         -6.9368e-01,  1.6455e+00, -8.0299e-01,  1.3514e+00, -2.7592e-01,\n",
       "         -1.5108e+00,  2.1048e+00,  2.7630e+00, -1.7465e+00,  1.4516e+00,\n",
       "         -1.5103e+00,  8.2115e-01, -2.1153e-01,  7.7890e-01,  1.5333e+00,\n",
       "          1.6097e+00, -4.0323e-01],\n",
       "        [-8.3447e-01,  5.9780e-01, -5.1406e-02, -6.4559e-02, -4.9701e-01,\n",
       "          4.6576e-01, -2.5726e-01, -1.0673e+00,  2.0089e+00, -5.3698e-01,\n",
       "          2.2280e-01,  6.9705e-01, -1.4267e+00,  9.0594e-01,  1.4459e-01,\n",
       "          2.2800e-01,  2.4900e+00, -1.2237e+00,  1.0107e+00,  5.5600e-01,\n",
       "         -1.5935e+00, -1.2706e+00,  6.9033e-01, -1.9614e-01,  3.4491e-01,\n",
       "         -3.4189e-01,  4.7587e-01, -7.6634e-01, -4.1896e-01, -4.3699e-01,\n",
       "         -1.0012e+00, -4.0943e-01],\n",
       "        [-1.6669e+00, -1.3651e+00, -1.6552e-01,  9.6225e-01,  3.1549e-02,\n",
       "         -7.4190e-01, -2.9779e-01,  1.7166e-02, -1.7722e-01, -1.3343e-01,\n",
       "          2.9396e-01,  1.3850e+00,  1.2091e-01,  2.5418e+00, -6.4046e-01,\n",
       "         -1.9740e+00, -3.2957e-01,  7.9589e-03,  9.2623e-01, -1.8846e+00,\n",
       "          1.6696e-01,  4.5862e-01, -1.7662e+00,  5.8599e-01,  1.7510e+00,\n",
       "          2.8072e-01,  3.1096e-01, -6.5376e-01, -6.5763e-01,  3.1845e-01,\n",
       "         -5.4959e-01, -1.4649e+00],\n",
       "        [-2.0555e+00,  1.8275e+00,  1.3035e+00, -4.5013e-01,  1.3471e+00,\n",
       "          1.6910e+00, -1.2445e-01, -1.6824e+00, -2.6608e-02,  7.4049e-02,\n",
       "          1.0517e+00,  6.7789e-01,  3.0665e-01, -7.4723e-01,  7.4349e-01,\n",
       "          8.8766e-01,  2.2874e+00,  9.6114e-01, -1.5297e+00, -2.9122e-01,\n",
       "         -1.1395e-01, -3.1367e-01, -6.2931e-01,  1.1385e+00, -9.9127e-01,\n",
       "          1.6999e-01,  1.2249e+00, -2.3454e-01, -1.0572e+00, -6.5427e-01,\n",
       "          1.5909e+00, -6.9949e-01],\n",
       "        [-8.9606e-01,  6.6191e-02, -5.6280e-02,  2.3412e+00, -2.7234e+00,\n",
       "          5.0967e-01, -8.1447e-01, -2.4604e-01,  4.5085e-03,  2.0474e+00,\n",
       "         -1.5755e-01, -2.1867e-01, -1.3519e+00, -5.7281e-02, -1.8540e+00,\n",
       "         -1.3849e+00, -3.4540e-01, -1.1625e+00,  1.4448e-01,  1.6632e-01,\n",
       "          7.5070e-01,  9.1317e-01, -1.7277e+00,  1.3055e+00,  9.5932e-01,\n",
       "          1.0600e+00,  6.2986e-01, -1.2867e+00, -6.8748e-01,  2.1382e+00,\n",
       "          5.1141e-01,  1.2191e+00],\n",
       "        [ 1.9098e-01, -3.4251e-01,  1.7955e+00,  1.3915e+00,  1.0785e+00,\n",
       "         -6.1495e-01, -4.5885e-01,  5.6748e-01,  1.8289e-02, -1.6608e+00,\n",
       "          1.1169e+00,  5.1965e-01, -1.2423e+00, -9.6182e-01, -8.4998e-02,\n",
       "          1.1854e-01,  2.9843e-01, -7.2636e-01, -3.1187e-01, -4.5604e-01,\n",
       "          6.4407e-01,  6.0728e-01,  1.2397e+00,  7.3249e-01,  5.0418e-01,\n",
       "          8.7135e-01, -2.7416e-01, -7.4689e-01, -5.8324e-01,  3.6988e-01,\n",
       "         -5.5563e-01, -3.9828e-01],\n",
       "        [-5.8188e-01, -2.2083e-01,  1.3537e-02, -3.0574e-01, -3.0384e-02,\n",
       "          8.2161e-01,  3.8670e-04, -4.4742e-01,  8.2040e-01, -1.5178e+00,\n",
       "          6.1587e-01, -1.8648e+00, -9.7773e-01,  6.3224e-02, -4.5483e-01,\n",
       "         -4.1474e-01,  1.4987e+00, -3.9867e-02, -8.0510e-01, -1.1624e+00,\n",
       "          4.2716e-01, -2.8192e-01, -1.2773e-02, -8.7792e-01, -3.2248e-01,\n",
       "          1.8299e-01, -9.3030e-01, -1.2488e+00,  1.1192e+00, -1.9079e+00,\n",
       "         -5.2756e-01,  1.0807e+00]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "32eb2668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8077e-01, -6.9988e-02, -3.5962e-01, -9.1520e-01,  6.2577e-01,\n",
       "          2.5510e-02,  9.5451e-01,  6.4349e-02,  3.6115e-01,  1.1679e+00,\n",
       "         -1.3499e+00, -5.1018e-01,  2.3596e-01, -2.3978e-01, -9.2111e-01,\n",
       "          1.5433e+00,  1.3488e+00, -1.3964e-01,  2.8580e-01,  9.6512e-01,\n",
       "         -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,  1.2603e-01,\n",
       "         -1.5627e+00, -1.1601e+00, -3.3484e-01,  4.4777e-01, -8.0164e-01,\n",
       "          1.5236e+00,  2.5086e+00],\n",
       "        [-2.4116e-01, -1.6063e-01,  3.2526e-01, -3.9683e-01,  3.9208e-01,\n",
       "          5.7976e-01, -9.9692e-02, -1.1702e-01, -7.3193e-02,  1.2198e-01,\n",
       "         -4.0159e-01, -1.0025e+00, -4.8488e-01,  1.6602e-01, -7.5923e-01,\n",
       "          4.2481e-01,  1.4972e+00, -4.7132e-01,  8.1860e-01,  3.4460e-01,\n",
       "         -1.7740e+00,  1.2990e+00,  2.1250e+00, -5.7775e-01,  7.8881e-01,\n",
       "         -1.5365e+00, -1.6947e-01, -2.7318e-01,  6.1334e-01,  3.6581e-01,\n",
       "          1.5667e+00,  1.0527e+00],\n",
       "        [-4.3893e-01,  9.2179e-02,  1.9971e-01, -2.8607e-01,  9.5720e-02,\n",
       "          5.4176e-01, -1.5221e-01, -4.3377e-01,  6.2085e-01, -9.7673e-02,\n",
       "         -1.9346e-01, -4.3597e-01, -7.9881e-01,  4.1266e-01, -4.5796e-01,\n",
       "          3.5921e-01,  1.8281e+00, -7.2210e-01,  8.8263e-01,  4.1507e-01,\n",
       "         -1.7138e+00,  4.4242e-01,  1.6468e+00, -4.5054e-01,  6.4084e-01,\n",
       "         -1.1383e+00,  4.5641e-02, -4.3757e-01,  2.6924e-01,  9.8210e-02,\n",
       "          7.1071e-01,  5.6531e-01],\n",
       "        [-7.4593e-01, -2.7215e-01,  1.0840e-01,  2.6009e-02,  7.9677e-02,\n",
       "          2.2085e-01, -1.8861e-01, -3.2104e-01,  4.2133e-01, -1.0661e-01,\n",
       "         -7.1608e-02,  1.9264e-02, -5.6888e-01,  9.4494e-01, -5.0358e-01,\n",
       "         -2.2410e-01,  1.2887e+00, -5.3959e-01,  8.9353e-01, -1.5984e-01,\n",
       "         -1.2436e+00,  4.4647e-01,  7.9352e-01, -1.9141e-01,  9.1839e-01,\n",
       "         -7.8353e-01,  1.1197e-01, -4.9162e-01,  3.7521e-02,  1.5327e-01,\n",
       "          3.9564e-01,  5.7752e-02],\n",
       "        [-1.0078e+00,  1.4777e-01,  3.4742e-01, -6.9219e-02,  3.3317e-01,\n",
       "          5.1487e-01, -1.7578e-01, -5.9330e-01,  3.3174e-01, -7.0480e-02,\n",
       "          1.5306e-01,  1.5099e-01, -3.9377e-01,  6.0651e-01, -2.5417e-01,\n",
       "         -1.7461e-03,  1.4884e+00, -2.3944e-01,  4.0888e-01, -1.8612e-01,\n",
       "         -1.0177e+00,  2.9444e-01,  5.0895e-01,  7.4572e-02,  5.3646e-01,\n",
       "         -5.9283e-01,  3.3455e-01, -4.4020e-01, -1.8142e-01, -8.2378e-03,\n",
       "          6.3469e-01, -9.3697e-02],\n",
       "        [-9.8921e-01,  1.3417e-01,  2.8014e-01,  3.3252e-01, -1.7627e-01,\n",
       "          5.1401e-01, -2.8222e-01, -5.3542e-01,  2.7721e-01,  2.8251e-01,\n",
       "          1.0129e-01,  8.9380e-02, -5.5346e-01,  4.9588e-01, -5.2080e-01,\n",
       "         -2.3227e-01,  1.1828e+00, -3.9329e-01,  3.6482e-01, -1.2738e-01,\n",
       "         -7.2296e-01,  3.9757e-01,  1.3617e-01,  2.7973e-01,  6.0694e-01,\n",
       "         -3.1736e-01,  3.8377e-01, -5.8128e-01, -2.6576e-01,  3.4950e-01,\n",
       "          6.1414e-01,  1.2510e-01],\n",
       "        [-8.2062e-01,  6.6077e-02,  4.9662e-01,  4.8380e-01,  2.9884e-03,\n",
       "          3.5273e-01, -3.0746e-01, -3.7787e-01,  2.4022e-01,  4.8829e-03,\n",
       "          2.4638e-01,  1.5085e-01, -6.5187e-01,  2.8763e-01, -4.5855e-01,\n",
       "         -1.8215e-01,  1.0565e+00, -4.4087e-01,  2.6815e-01, -1.7433e-01,\n",
       "         -5.2767e-01,  4.2752e-01,  2.9381e-01,  3.4441e-01,  5.9226e-01,\n",
       "         -1.4755e-01,  2.8978e-01, -6.0494e-01, -3.1111e-01,  3.5242e-01,\n",
       "          4.4703e-01,  5.0332e-02],\n",
       "        [-7.9077e-01,  3.0213e-02,  4.3624e-01,  3.8511e-01, -1.1831e-03,\n",
       "          4.1134e-01, -2.6898e-01, -3.8656e-01,  3.1274e-01, -1.8545e-01,\n",
       "          2.9257e-01, -1.0111e-01, -6.9260e-01,  2.5958e-01, -4.5808e-01,\n",
       "         -2.1123e-01,  1.1117e+00, -3.9074e-01,  1.3399e-01, -2.9784e-01,\n",
       "         -4.0832e-01,  3.3884e-01,  2.5549e-01,  1.9162e-01,  4.7791e-01,\n",
       "         -1.0623e-01,  1.3727e-01, -6.8542e-01, -1.3233e-01,  6.9874e-02,\n",
       "          3.2521e-01,  1.7912e-01]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bag_of_words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f418cbd3",
   "metadata": {},
   "source": [
    "That was inefficient, can make it much better using matrix multiplication. More specifically, we can use ```torch.tril()``` to create a lower triangular matrix. This is the trick to block out the future tokens from the current token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "58744c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tril(torch.ones(T,T))\n",
    "weights = weights / weights.sum(1, keepdim=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0aefbee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8077e-01, -6.9988e-02, -3.5962e-01,  ..., -8.0164e-01,\n",
       "           1.5236e+00,  2.5086e+00],\n",
       "         [-2.4116e-01, -1.6063e-01,  3.2526e-01,  ...,  3.6581e-01,\n",
       "           1.5667e+00,  1.0527e+00],\n",
       "         [-4.3893e-01,  9.2179e-02,  1.9971e-01,  ...,  9.8210e-02,\n",
       "           7.1071e-01,  5.6531e-01],\n",
       "         ...,\n",
       "         [-9.8921e-01,  1.3417e-01,  2.8014e-01,  ...,  3.4950e-01,\n",
       "           6.1414e-01,  1.2510e-01],\n",
       "         [-8.2062e-01,  6.6077e-02,  4.9662e-01,  ...,  3.5242e-01,\n",
       "           4.4703e-01,  5.0332e-02],\n",
       "         [-7.9077e-01,  3.0213e-02,  4.3624e-01,  ...,  6.9874e-02,\n",
       "           3.2521e-01,  1.7912e-01]],\n",
       "\n",
       "        [[ 4.5618e-01, -1.0917e+00, -8.2073e-01,  ...,  5.1187e-02,\n",
       "          -6.5764e-01, -2.5729e+00],\n",
       "         [ 2.3859e-01, -4.2831e-02, -1.0349e+00,  ...,  4.1852e-01,\n",
       "          -9.0388e-01, -6.2984e-01],\n",
       "         [ 8.9262e-01, -1.0170e-01, -5.0905e-01,  ...,  6.4184e-02,\n",
       "          -2.4146e-01, -6.8638e-01],\n",
       "         ...,\n",
       "         [ 5.6778e-01,  3.5322e-01, -2.6951e-01,  ...,  2.4809e-01,\n",
       "          -5.0422e-02, -9.0907e-02],\n",
       "         [ 5.0014e-01,  2.5768e-01, -1.1887e-01,  ..., -1.2143e-02,\n",
       "           1.5501e-01,  2.5669e-02],\n",
       "         [ 5.1830e-01,  1.8405e-01, -2.3389e-01,  ...,  5.5685e-04,\n",
       "           8.8793e-02, -3.7308e-02]],\n",
       "\n",
       "        [[-6.0669e-01,  1.8328e+00,  2.9308e-01,  ...,  1.0041e+00,\n",
       "           8.6564e-01,  1.6879e-01],\n",
       "         [-4.2097e-01,  7.8707e-01,  1.5309e-01,  ...,  8.3654e-01,\n",
       "           8.0955e-01, -1.8358e-01],\n",
       "         [-6.2322e-01,  7.0294e-01, -1.6321e-01,  ...,  9.1471e-01,\n",
       "           7.0306e-01, -2.8491e-01],\n",
       "         ...,\n",
       "         [-3.3422e-01, -8.5831e-02,  2.4132e-01,  ...,  2.3987e-01,\n",
       "           1.5860e-02, -2.0914e-04],\n",
       "         [-1.3180e-01,  9.1034e-02,  2.8111e-01,  ...,  2.7661e-01,\n",
       "           2.1950e-02,  1.4572e-02],\n",
       "         [-1.0633e-01,  2.1815e-01,  3.3713e-01,  ...,  2.9162e-01,\n",
       "          -1.0273e-01,  7.6774e-02]],\n",
       "\n",
       "        [[ 3.3299e-01,  1.0995e+00,  4.0335e-01,  ...,  1.6634e+00,\n",
       "          -4.7180e-01,  5.8567e-01],\n",
       "         [-3.1246e-01,  1.0215e+00, -8.9794e-01,  ...,  4.6688e-01,\n",
       "          -1.5327e-01,  1.2333e-01],\n",
       "         [ 3.0558e-01,  1.0220e+00, -1.5221e-01,  ...,  2.9682e-01,\n",
       "          -1.8568e-01, -1.6754e-01],\n",
       "         ...,\n",
       "         [ 4.9366e-01,  5.0418e-01,  1.2845e-01,  ...,  1.5478e-01,\n",
       "           6.3110e-01, -1.5307e-01],\n",
       "         [ 4.2775e-01,  3.4162e-01,  1.5207e-01,  ...,  8.5563e-02,\n",
       "           5.3821e-01,  5.8159e-04],\n",
       "         [ 4.8912e-01,  3.3639e-01,  2.0939e-01,  ...,  1.7840e-01,\n",
       "           4.1060e-01,  2.2963e-01]]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batched matrix multiply\n",
    "# shapes are not the same, pytorch adds batch dimension\n",
    "# original: (T,T) @ (B,T,C) --> ???\n",
    "# updated:(B,T,T) @ (B,T,C) --> (B,T,C)\n",
    "x_bag_of_words_2 = weights @ x \n",
    "x_bag_of_words_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5e515494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_bag_of_words and x_bag_of_words_2 are the same now\n",
    "# Using the matrix way is significantly faster, however\n",
    "torch.allclose(x_bag_of_words, x_bag_of_words_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0547b86",
   "metadata": {},
   "source": [
    "However, this is the best way to do the lower triangular batched matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "17c2be13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "weights = torch.zeros((T,T))\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "710a2253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make all zero weights negative infinity\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ffc443",
   "metadata": {},
   "source": [
    "### Aha-moment:\n",
    "This is super useful in self attention, because this weights matrix tells us how much of each vector to pay attention to! We can train this to get specialized values, but also making sure the future can't communicate the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9a4076df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We then take softmax() along every row\n",
    "# https://en.wikipedia.org/wiki/Softmax_function\n",
    "# This is just normalization, which creates the weight matrix\n",
    "\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e529b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final bag of words\n",
    "out = weights @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "10856a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30090933",
   "metadata": {},
   "source": [
    "### Crux of Self Attention\n",
    "\n",
    "We should just use uniform weights, each equal and add up to one in our weights matrix. Some words want to pay more attention to others, such as how a vowel might pay attention to different consonants with differing weights. We still want to gather information from the past, but in a data dependent way. (Encoders don't use ```torch.tril()```, decoders do)\n",
    "\n",
    "How does self attention solve this?\n",
    "Every single token at each position emits two vectors:\n",
    "1. Query\n",
    "2. Key\n",
    "\n",
    "What does the query vector do?\n",
    "It represents what we are looking for. Like a google search query or a SQL query.\n",
    "\n",
    "That does the key vector do?\n",
    "It represents what the weights contain.\n",
    "\n",
    "To find the similarity, or affinity between these two vectors, we simply do a dot product between the keys and the queries. If the key and query are aligned, then they interact a very high amount, and it pays more attention to that specific token. \n",
    "\n",
    "We can do that using a single self-attention head.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "25c6f430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# single head attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "\n",
    "k = key(x)                       # (B,T,16)\n",
    "q = query(x)                     # (B,T,16)\n",
    "weights = q @ k.transpose(-2,-1) # (B,T,16) @ (B,16,T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "# weights = torch.zeros((T,T))\n",
    "weights = weights.masked_fill(tril==0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = weights @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "69329ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bfd7a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c7ae18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
